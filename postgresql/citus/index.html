<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PostgreSQL Citus 간단히 살펴보기 | kimDuBiA</title><meta name=keywords content="postgresql,citus,sharding,architecture"><meta name=description content="2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.
깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..
현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.
목차  Citus 란? HA 방식에 따른 아키텍처 성능 테스트 Citus 설치 방법 Query processing Distributed deadlock Query plan 참고 내용  Citus -. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )"><meta name=author content="kimdubi"><link rel=canonical href=/postgresql/citus/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.2dbef8664bbfb3e83a0a44fd4a8fc5010240dbcd1dbc1400753b928b497b8f5e.css integrity="sha256-Lb74Zku/s+g6CkT9So/FAQJA280dvBQAdTuSi0l7j14=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="PostgreSQL Citus 간단히 살펴보기"><meta property="og:description" content="2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.
깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..
현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.
목차  Citus 란? HA 방식에 따른 아키텍처 성능 테스트 Citus 설치 방법 Query processing Distributed deadlock Query plan 참고 내용  Citus -. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )"><meta property="og:type" content="article"><meta property="og:url" content="/postgresql/citus/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="postgresql"><meta property="article:published_time" content="2021-10-06T00:31:52+09:00"><meta property="article:modified_time" content="2021-10-06T00:31:52+09:00"><meta property="og:site_name" content="kimDuBiA"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="PostgreSQL Citus 간단히 살펴보기"><meta name=twitter:description content="2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.
깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..
현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.
목차  Citus 란? HA 방식에 따른 아키텍처 성능 테스트 Citus 설치 방법 Query processing Distributed deadlock Query plan 참고 내용  Citus -. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Postgresqls","item":"/postgresql/"},{"@type":"ListItem","position":3,"name":"PostgreSQL Citus 간단히 살펴보기","item":"/postgresql/citus/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PostgreSQL Citus 간단히 살펴보기","name":"PostgreSQL Citus 간단히 살펴보기","description":"2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.\n깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..\n현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.\n목차  Citus 란? HA 방식에 따른 아키텍처 성능 테스트 Citus 설치 방법 Query processing Distributed deadlock Query plan 참고 내용  Citus -. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )","keywords":["postgresql","citus","sharding","architecture"],"articleBody":" 2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.\n깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..\n현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.\n목차  Citus 란? HA 방식에 따른 아키텍처 성능 테스트 Citus 설치 방법 Query processing Distributed deadlock Query plan 참고 내용  Citus -. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )\n-. hash 기반으로 테이블을 분산함\n-. 엔터프라이즈 버전은 자동 리밸런싱 + 커넥션 풀 지원\n-. node 간 HA 별도 구축 필요 -. 별도의 백업정책 필요함\n-. mongodb sharding + mysql spider engine 아키텍처와 유사\ncoordinator : distributed table에 대한 meta data 관리, 클라이언트의 요청을 워커노드에 전달하는 역할 * query 최종 sorting 작업 수행 worker node : coordinator로 부터 요청을 받아 실제 데이터를 처리한 후 coordinator 에게 전달하는 역할 * dml, ddl, analyze, vacuum 등 coordinator 로 부터 전달 받은 커맨드 실행  HA 방식에 따른 아키텍처   shard_replication_factor\n= worker node down 에 대비하여 분산 테이블을 다른 worker node에도 복제하는 방식\n구축은 쉬우나 worker node down 시 일부 dml 불가 현상 및 수동으로 sync 맞춰줘야 하는 단점이 있음\n  replication + pgpool+master_add_secondary_node\n= Primary node down 시 auto-failover 된 standby node가 투입되는 방식\n  streaming replication : postgresql 의 복제 방식으로 mysql처럼 sql thread /io thread 방식과 유사함 (PostgreSQL에서는 sender / receiver, sql 커맨드 대신 트랜잭션로그)\n  pgpool : auto - failover\n  성능 테스트 single node와 citus - 3 worker nodes 아키텍처 (azure) 간 성능 테스트\ndata load  single  citus= \\copy github_events FROM large_events.csv with csv COPY 1146625 Time: 1451256.144 ms (24:11.256)  citus  citus= \\copy github_events from 'large_events.csv' with csv; COPY 1146625 Time: 879733.291 ms (14:39.733) index 생성  single  citus= create index test_idx on github_events using gin(payload jsonb_path_ops); CREATE INDEX Time: 193734.521 ms (03:13.735)  citus  citus= create index test_idx on github_events using gin(payload jsonb_path_ops); CREATE INDEX Time: 49713.177 ms (00:49.713) select  single  SELECT date_trunc('minute', created_at) AS minute, sum((payload-'distinct_size')::int) AS num_commits FROM github_events WHERE event_type = 'PushEvent' GROUP BY minute ORDER BY minute; Time: 1213.770 ms (00:01.214)  citus  SELECT date_trunc('minute', created_at) AS minute, sum((payload-'distinct_size')::int) AS num_commits FROM github_events WHERE event_type = 'PushEvent' GROUP BY minute ORDER BY minute; Time: 847.792 ms 구성 방법 모든 노드에 citus 설치 docker network create citus-test docker run --name citus_master -d --network citus-test citusdata/citus docker run --name citus_slave -d --network citus-test citusdata/citus docker run --name citus_worker1 -d --network citus-test citusdata/citus docker run --name citus_worker2 -d --network citus-test citusdata/citus docker run --name citus_worker3 -d --network citus-test citusdata/citus docker run --name citus_worker1_standby -d --network citus-test citusdata/citus docker run --name citus_worker2_standby -d --network citus-test citusdata/citus docker run --name citus_worker3_standby -d --network citus-test citusdata/citus  = primary worker node 3대 + standby worker node 3대 + coordinator M/S 하나씩\nstreaming replication + pgpool 로 HA 구성\npostgres=# select * from pg_available_extensions where name='citus'; name | default_version | installed_version | comment -------+-----------------+-------------------+---------------------------- citus | 8.3-1 | 8.3-1 | Citus distributed database (1 row)  = citus extension 설치 (extension 은 mysql 의 plugin 개념)\npg_hba.conf 에 노드 추가 postgres@07a3ccbd0898:~/data$ tail -5f pg_hba.conf host replication all 172.19.0.0/24 trust host all all 172.19.0.0/24 trust postgres@07a3ccbd0898:~$ /usr/lib/postgresql/11/bin/pg_ctl reload -D /var/lib/postgresql/data server signaled  = oracle 의 sqlnet.ora, cubrid iplist , mysql ip/host 개념처럼 해당 db에 붙을 수 있는 host 허용하는 부분\ncoordinator node 에 워커노드 추가하기 postgres=# SELECT master_add_node ('citus_worker1',5432); master_add_node ------------------------------------------------------ (7,7,citus_worker1,5432,default,f,t,primary,default) (1 row) postgres=# SELECT master_add_node ('citus_worker2',5432); master_add_node ------------------------------------------------------ (8,8,citus_worker2,5432,default,f,t,primary,default) (1 row) postgres=# SELECT master_add_node ('citus_worker3',5432); master_add_node ------------------------------------------------------ (9,9,citus_worker3,5432,default,f,t,primary,default) (1 row) postgres=# select master_add_secondary_node('citus_worker1_standby','5432','citus_worker1','5432'); master_add_secondary_node ----------------------------------------------------------------- (10,7,citus_worker1_standby,5432,default,f,t,secondary,default) (1 row) postgres=# select master_add_secondary_node('citus_worker2_standby','5432','citus_worker2','5432'); master_add_secondary_node ----------------------------------------------------------------- (10,7,citus_worker2_standby,5432,default,f,t,secondary,default) (1 row) postgres=# select master_add_secondary_node('citus_worker3_standby','5432','citus_worker3','5432'); master_add_secondary_node ----------------------------------------------------------------- (10,7,citus_worker3_standby,5432,default,f,t,secondary,default) (1 row)  = hostname,port 로 worker node, standby worker node 추가\n추가된 워커노드 확인 postgres=# select * from pg_dist_node; nodeid | groupid | nodename | nodeport | noderack | hasmetadata | isactive | noderole | nodecluster --------+---------+-----------------------+----------+----------+-------------+----------+-----------+------------- 7 | 7 | citus_worker1 | 5432 | default | f | t | primary | default 8 | 8 | citus_worker2 | 5432 | default | f | t | primary | default 9 | 9 | citus_worker3 | 5432 | default | f | t | primary | default 10 | 7 | citus_worker1_standby | 5432 | default | f | t | secondary | default 11 | 8 | citus_worker2_standby | 5432 | default | f | t | secondary | default 12 | 9 | citus_worker3_standby | 5432 | default | f | t | secondary | default (6 rows)  distributed table 생성  citus coordinator 에서 수행  postgres=# set citus.shard_count=6; SET # shard_replication_factor 사용할 땐 아래 설정 추가 postgres=# set citus.shard_replication_factor=2; SET  citus.shard_count : 해당 테이블을 몇개로 쪼갤 것인지 설정 (default 32) citus.shard_replication_factor : 쪼개진 테이블의 복사본을 몇개 갖고 있을 것인지 설정 (default 1) ( shard_count 6 * replication_factor 2 ) / worker node 3 = 4 = 한 노드에 쪼개진 테이블 4개씩 갖고있게됨  CREATE TABLE companies ( id bigint NOT NULL, name text NOT NULL, image_url text, created_at timestamp without time zone NOT NULL, updated_at timestamp without time zone NOT NULL ); CREATE TABLE campaigns ( id bigint NOT NULL, company_id bigint NOT NULL, name text NOT NULL, cost_model text NOT NULL, state text NOT NULL, monthly_budget bigint, blacklisted_site_urls text[], created_at timestamp without time zone NOT NULL, updated_at timestamp without time zone NOT NULL ); CREATE TABLE ads ( id bigint NOT NULL, company_id bigint NOT NULL, campaign_id bigint NOT NULL, name text NOT NULL, image_url text, target_url text, impressions_count bigint DEFAULT 0, clicks_count bigint DEFAULT 0, created_at timestamp without time zone NOT NULL, updated_at timestamp without time zone NOT NULL ); = test 용 테이블 생성\nSELECT create_distributed_table('companies', 'id'); SELECT create_distributed_table('campaigns', 'company_id'); SELECT create_distributed_table('ads', 'company_id');  = 생성한 테이블을 각각 id, company_id, company_id 기준으로 distributed table 생성\ncoordinator postgres=# \\dt List of relations Schema | Name | Type | Owner --------+------------------+-------+---------- public | ads | table | postgres public | campaigns | table | postgres public | companies | table | postgres   worker node 1  public | campaigns_102174 | table | postgres public | campaigns_102176 | table | postgres public | campaigns_102178 | table | postgres  worker node 2  public | campaigns_102175 | table | postgres public | campaigns_102177 | table | postgres public | campaigns_102179 | table | postgres postgres=# select a.logicalrelid,a.shardid,a.shardminvalue,a.shardmaxvalue,b.shardstate,b.groupid from pg_dist_shard a inner join pg_dist_placement b on a.shardid=b.shardid where a.logicalrelid=CAST('campaigns' as regclass); logicalrelid | shardid | shardminvalue | shardmaxvalue | shardstate | groupid --------------+---------+---------------+---------------+------------+--------- campaigns | 102174 | -2147483648 | -1431655767 | 1 | 1 campaigns | 102175 | -1431655766 | -715827885 | 1 | 2 campaigns | 102176 | -715827884 | -3 | 1 | 1 campaigns | 102177 | -2 | 715827879 | 1 | 2 campaigns | 102178 | 715827880 | 1431655761 | 1 | 1 campaigns | 102179 | 1431655762 | 2147483647 | 1 | 2  = 생성된 distributed table 과 매핑되는 hash value range\nQuery processiong 과정 postgres=# SELECT master_get_table_metadata('test'); master_get_table_metadata ------------------------------ (16759,t,h,t,1,1073741824,2) (1 row) postgres=#select * From pg_backend_pid(); pg_backend_pid ---------------- 796 (1 row) postgres=# select * from test;  = 디버깅 대상 세션 준비\n[root@1a99d5cd2d41 /]# gdb -p 796 (gdb) b CreateDistributedPlan Breakpoint 1 at 0x7fc3a0bf9e20: file planner/distributed_planner.c, line 575. (gdb) b ExecProcNode Breakpoint 2 at 0x5f3225: ExecProcNode. (55 locations) (gdb) b IsDistributedTable Breakpoint 3 at 0x7fc3a0c25560: file utils/metadata_cache.c, line 269. (gdb) b CitusExecutorRun Breakpoint 4 at 0x7fc3a0be9580: file executor/multi_executor.c, line 108. (gdb) b CitusExecutorStart Breakpoint 5 at 0x7fc3a0be9480: file executor/multi_executor.c, line 64.  = gdb breakpoint 설정\nBreakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269 269\t{ (gdb) bt #0 IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269 #1 0x00007fc3a0bf9963 in ListContainsDistributedTableRTE (rangeTableList=rangeTableList@entry=0x1961150) at planner/distributed_planner.c:289 #2 0x00007fc3a0bfa525 in distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0) at planner/distributed_planner.c:117 #3 0x00000000007261fc in pg_plan_query (querytree=querytree@entry=0x1960a68, cursorOptions=cursorOptions@entry=256, boundParams=boundParams@entry=0x0) at postgres.c:832 #4 0x00000000007262de in pg_plan_queries (querytrees=, cursorOptions=cursorOptions@entry=256, boundParams=boundParams@entry=0x0) at postgres.c:898 #5 0x000000000072674a in exec_simple_query (query_string=0x195fc10 \"select * from test;\") at postgres.c:1073 #6 0x00000000007278c2 in PostgresMain (argc=, argv=argv@entry=0x19b2480, dbname=0x19b2328 \"postgres\", username=) at postgres.c:4182 #7 0x000000000047b0cf in BackendRun (port=0x19aa6c0) at postmaster.c:4358 #8 BackendStartup (port=0x19aa6c0) at postmaster.c:4030 #9 ServerLoop () at postmaster.c:1707 #10 0x00000000006be989 in PostmasterMain (argc=argc@entry=3, argv=argv@entry=0x195b960) at postmaster.c:1380 #11 0x000000000047bb11 in main (argc=3, argv=0x195b960) at main.c:228  = “select * from test” parsing 후 distributed_planner 에서 해당 테이블이 distributed table인지 check ((relationId=16759)\nBreakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269 269\t{ (gdb) bt #0 CreateDistributedPlan (planId=planId@entry=5, originalQuery=originalQuery@entry=0x1960958, query=query@entry=0x1960a68, boundParams=boundParams@entry=0x0, hasUnresolvedParams=hasUnresolvedParams@entry=false, plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/distributed_planner.c:575 #1 0x00007fc3a0bfa34d in CreateDistributedPlannedStmt (plannerRestrictionContext=0x1961608, boundParams=0x0, query=0x1960a68, originalQuery=0x1960958, localPlan=0x19af2c0, planId=5) at planner/distributed_planner.c:493 #2 distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0) at planner/distributed_planner.c:185  = distributed plan 생성 시작\nBreakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269 269\t{ (gdb) bt #0 IsDistributedTable (relationId=relationId@entry=16759) at utils/metadata_cache.c:269 #1 0x00007fc3a0c0a771 in MultiRouterPlannableQuery (query=query@entry=0x1960a68) at planner/multi_router_planner.c:3002 #2 0x00007fc3a0c0cc61 in CreateRouterPlan (originalQuery=originalQuery@entry=0x1960958, query=query@entry=0x1960a68, plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/multi_router_planner.c:179 #3 0x00007fc3a0bf9ebe in CreateDistributedPlan (planId=planId@entry=5, originalQuery=originalQuery@entry=0x1960958, query=query@entry=0x1960a68, boundParams=boundParams@entry=0x0, hasUnresolvedParams=hasUnresolvedParams@entry=false, plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/distributed_planner.c:632 #4 0x00007fc3a0bfa34d in CreateDistributedPlannedStmt (plannerRestrictionContext=0x1961608, boundParams=0x0, query=0x1960a68, originalQuery=0x1960958, localPlan=0x19af2c0, planId=5) at planner/distributed_planner.c:493 #5 distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0) at planner/distributed_planner.c:185  = 어떤 worker node로 보낼지 plan 생성\nBreakpoint 5, CitusExecutorStart (queryDesc=0x1a78a20, eflags=0) at executor/multi_executor.c:64 64\t{ (gdb) bt #0 CitusExecutorStart (queryDesc=0x1a78a20, eflags=0) at executor/multi_executor.c:64  = 생성된 distributed plan 을 worker node에서 실행시킬 준비\nBreakpoint 4, CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0, execute_once=true) at executor/multi_executor.c:108 108\t{ (gdb) bt #0 CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0, execute_once=true) at executor/multi_executor.c:108 Breakpoint 2, ExecutePlan (execute_once=, dest=0x1b0d4d0, direction=ForwardScanDirection, numberTuples=0, sendTuples=true, operation=CMD_SELECT, use_parallel_mode=, planstate=0x1a717c0, estate=0x1a715b0) at execMain.c:1723 1723\tslot = ExecProcNode(planstate); (gdb) bt #0 ExecutePlan (execute_once=, dest=0x1b0d4d0, direction=ForwardScanDirection, numberTuples=0, sendTuples=true, operation=CMD_SELECT, use_parallel_mode=, planstate=0x1a717c0, estate=0x1a715b0) at execMain.c:1723 #1 standard_ExecutorRun (queryDesc=queryDesc@entry=0x1a78a20, direction=direction@entry=ForwardScanDirection, count=count@entry=0, execute_once=execute_once@entry=true) at execMain.c:364 #2 0x00007fc3a0be95d3 in CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0, execute_once=) at executor/multi_executor.c:151  = 각각의 worker node 에서 query 수행\nDistributed Deadlock coordinator | session 1 | session 2 | | --- | --- | | update test set b='session1' where a=1;| | | | update test set b='session2' where a=2; | | update test set b='session1' where a=2; | | | | update test set b='session2' where a=1; |  worker node 관점 | worker1 | | worker2 | | | --- | --- | --- | --- | | update test_worker1 set b='session1' where a=1; | holder | update test_worker2 set b='session2' where a=2; | holder | | update test_worker1 set b='session2' where a=1; | waiter | update test_worker2 set b='session1' where a=2; | waiter |  = 각 worker node가 봤을 때는 단순한 lock 대기상황\ncoordinator 관점 = session1의 트랜잭션은 worker node2 에서 session2 에 의해 대기,\nsession2 의 트랜잭션은 worker node1 에서 session1 에 의해 대기하는 deadlock 상황\n* session 1 (1,1) (gdb) info locals currentDistributedTransactionId = 0x10b5340 backendData = {databaseId = 12368, userId = 10, mutex = 1 '\\001', cancelledDueToDeadlock = false, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = {initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 16, timestamp = 622787261936074}} * session 2 (2,2) currentDistributedTransactionId = 0x10c0340 backendData = {databaseId = 12368, userId = 10, mutex = 1 '\\001', cancelledDueToDeadlock = true, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = { initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 17, timestamp = 622787449815181}} * session 1 (1,2) currentDistributedTransactionId = 0x10b5340 backendData = {databaseId = 12368, userId = 10, mutex = 1 '\\001', cancelledDueToDeadlock = false, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = {initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 16, timestamp = 622787261936074}} * session 2 (2,1) (gdb) info locals currentDistributedTransactionId = 0x10c0340 backendData = {databaseId = 12368, userId = 10, mutex = 1 '\\001', cancelledDueToDeadlock = true, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = { initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 17, timestamp = 622787449815181}}  = deadlock 발생구간에서 트랜잭션ID 가 큰 후순위 트랜잭션을 kill 해서 deadlock 처리함\n쿼리 실행계획 distributed table 간 JOIN set citus.explain_all_tasks = on ; SELECT campaigns.id, campaigns.name, campaigns.monthly_budget, sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks FROM ads, campaigns WHERE ads.company_id = campaigns.company_id AND campaigns.company_id=5 AND campaigns.state = 'running' GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget ORDER BY total_impressions, total_clicks; QUERY PLAN --------------------------------------------------------------------------------------------------------------- Custom Scan (Citus Adaptive) (cost=0.00..0.00 rows=0 width=0) Task Count: 1 Tasks Shown: All - Task Node: host=citus_worker3 port=5432 dbname=postgres - Sort (cost=49.87..49.88 rows=3 width=94) Sort Key: (sum(ads.impressions_count)), (sum(ads.clicks_count)) - HashAggregate (cost=49.81..49.85 rows=3 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Nested Loop (cost=0.00..46.54 rows=261 width=46) - Seq Scan on ads_102021 ads (cost=0.00..38.04 rows=87 width=24) Filter: (company_id = 5) - Materialize (cost=0.00..5.25 rows=3 width=38) - Seq Scan on campaigns_102015 campaigns (cost=0.00..5.23 rows=3 width=38) Filter: ((company_id = 5) AND (state = 'running'::text))   두 테이블 간 조인을 하는데 citus_worker3 한 노드에서만 데이터를 가져옴 동일한 distributed column ( company_id =5 )에 대해서는 어차피 해쉬값이 같기 때문에 한노드에 저장됨 명시적으로는 아래와 같이 설정 가능  SELECT create_distributed_table(‘event’, ‘tenant_id’); SELECT create_distributed_table(‘page’, ‘tenant_id’, colocate_with = ‘event’);    SELECT campaigns.id, campaigns.name, campaigns.monthly_budget, sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks FROM ads, campaigns WHERE ads.company_id = campaigns.company_id AND campaigns.company_id between 1 and 100 AND campaigns.state = 'running' GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget ORDER BY total_impressions, total_clicks; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------- Sort (cost=0.00..0.00 rows=0 width=0) Sort Key: pg_catalog.sum((pg_catalog.sum(remote_scan.total_impressions))), pg_catalog.sum((pg_catalog.sum(remote_scan.total_clicks))) - HashAggregate (cost=0.00..0.00 rows=0 width=0) Group Key: remote_scan.id, remote_scan.name, remote_scan.monthly_budget - Custom Scan (Citus Adaptive) (cost=0.00..0.00 rows=0 width=0) Task Count: 6 Tasks Shown: All - Task Node: host=citus_worker1 port=5432 dbname=postgres - HashAggregate (cost=185.41..186.35 rows=63 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=58.58..123.37 rows=4963 width=46) Hash Cond: (campaigns.company_id = ads.company_id) - Seq Scan on campaigns_102014 campaigns (cost=0.00..6.19 rows=63 width=38) Filter: ((company_id = 1) AND (company_id Hash (cost=41.59..41.59 rows=1359 width=24) - Seq Scan on ads_102020 ads (cost=0.00..41.59 rows=1359 width=24) - Task Node: host=citus_worker2 port=5432 dbname=postgres - HashAggregate (cost=136.52..137.12 rows=40 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=49.27..94.11 rows=3393 width=46) Hash Cond: (campaigns.company_id = ads.company_id) - Seq Scan on campaigns_102015 campaigns (cost=0.00..5.61 rows=40 width=38) Filter: ((company_id = 1) AND (company_id Hash (cost=35.23..35.23 rows=1123 width=24) - Seq Scan on ads_102021 ads (cost=0.00..35.23 rows=1123 width=24) - Task Node: host=citus_worker3 port=5432 dbname=postgres - HashAggregate (cost=161.65..162.44 rows=53 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=49.67..106.97 rows=4374 width=46) Hash Cond: (campaigns.company_id = ads.company_id) - Seq Scan on campaigns_102016 campaigns (cost=0.00..5.68 rows=53 width=38) Filter: ((company_id = 1) AND (company_id Hash (cost=35.41..35.41 rows=1141 width=24) - Seq Scan on ads_102022 ads (cost=0.00..35.41 rows=1141 width=24) - Task Node: host=citus_worker1 port=5432 dbname=postgres - HashAggregate (cost=156.06..156.83 rows=51 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=47.91..103.41 rows=4212 width=46) Hash Cond: (campaigns.company_id = ads.company_id) - Seq Scan on campaigns_102017 campaigns (cost=0.00..5.61 rows=51 width=38) Filter: ((company_id = 1) AND (company_id Hash (cost=34.07..34.07 rows=1107 width=24) - Seq Scan on ads_102023 ads (cost=0.00..34.07 rows=1107 width=24) - Task Node: host=citus_worker2 port=5432 dbname=postgres - HashAggregate (cost=161.72..162.56 rows=56 width=95) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=6.13..103.98 rows=4619 width=47) Hash Cond: (ads.company_id = campaigns.company_id) - Seq Scan on ads_102024 ads (cost=0.00..31.22 rows=1022 width=24) - Hash (cost=5.43..5.43 rows=56 width=39) - Seq Scan on campaigns_102018 campaigns (cost=0.00..5.43 rows=56 width=39) Filter: ((company_id = 1) AND (company_id Task Node: host=citus_worker3 port=5432 dbname=postgres - HashAggregate (cost=215.04..216.08 rows=69 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Hash Join (cost=70.27..144.78 rows=5621 width=46) Hash Cond: (campaigns.company_id = ads.company_id) - Seq Scan on campaigns_102019 campaigns (cost=0.00..7.61 rows=69 width=38) Filter: ((company_id = 1) AND (company_id Hash (cost=50.12..50.12 rows=1612 width=24) - Seq Scan on ads_102025 ads (cost=0.00..50.12 rows=1612 width=24)  company_id 조건을 between 범위검색으로 변경 company_id =5 일 때 target query 로 처리되었던 것과는 달리 broad cast 로 모든 worker node 사용하게 됨 각각의 worker_node 에서 데이터 추출 후 최종적으로 coordinator에서 취합 \u0026 sorting 수행 mongodb 에서는 shard node 간 primary 선출, sorting 후 mongus 로 return 하는 것과는 달리 coordinator 에서 더 많은 역할을 수행함  distributed table 과 local table 간 JOIN SELECT campaigns.id, campaigns.name, campaigns.monthly_budget, sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks FROM local_ads ads, campaigns WHERE ads.company_id = campaigns.company_id AND campaigns.company_id=5 AND campaigns.state = 'running' GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget ORDER BY total_impressions, total_clicks; ERROR: relation local_ads is not distributed  = local table , 분산되지 않은 일반 테이블은 distributed table 과 조인이 불가능함.\n아래와 같이 inline view 로 풀어서 수행\nSELECT campaigns.id, campaigns.name, campaigns.monthly_budget, sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks FROM (select * from local_ads) ads, campaigns WHERE ads.company_id = campaigns.company_id AND campaigns.company_id=5 AND campaigns.state = 'running' GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget ORDER BY total_impressions, total_clicks; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Custom Scan (Citus Adaptive) (cost=0.00..0.00 rows=0 width=0) - Distributed Subplan 3_1 - Seq Scan on local_ads (cost=0.00..225.64 rows=7364 width=134) Task Count: 1 Tasks Shown: All - Task Node: host=citus_worker2 port=5432 dbname=postgres - Sort (cost=18.19..18.20 rows=3 width=94) Sort Key: (sum(intermediate_result.impressions_count)), (sum(intermediate_result.clicks_count)) - HashAggregate (cost=18.12..18.17 rows=3 width=94) Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget - Nested Loop (cost=0.00..17.93 rows=15 width=46) - Function Scan on read_intermediate_result intermediate_result (cost=0.00..12.50 rows=5 width=24) Filter: (company_id = 5) - Materialize (cost=0.00..5.25 rows=3 width=38) - Seq Scan on campaigns_102015 campaigns (cost=0.00..5.23 rows=3 width=38) Filter: ((company_id = 5) AND (state = 'running'::text))  참고 자료  https://docs.citusdata.com/en/v8.0/index.html https://www.youtube.com/watch?v=MwMNZxr-IZM\u0026t=1607s https://browndwarf.tistory.com/7?category=803646 https://github.com/citusdata/citus  ","wordCount":"2754","inLanguage":"en","datePublished":"2021-10-06T00:31:52+09:00","dateModified":"2021-10-06T00:31:52+09:00","author":{"@type":"Person","name":"kimdubi"},"mainEntityOfPage":{"@type":"WebPage","@id":"/postgresql/citus/"},"publisher":{"@type":"Organization","name":"kimDuBiA","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href accesskey=h title="Hi (Alt + H)">Hi</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/categories/ title=categories><span>categories</span></a></li><li><a href=/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/postgresql/>Postgresqls</a></div><h1 class=post-title>PostgreSQL Citus 간단히 살펴보기</h1><div class=post-meta>October 6, 2021&nbsp;·&nbsp;13 min&nbsp;·&nbsp;kimdubi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#%eb%aa%a9%ec%b0%a8 aria-label=목차>목차</a></li><li><a href=#citus aria-label=Citus>Citus</a></li><li><a href=#ha-%eb%b0%a9%ec%8b%9d%ec%97%90-%eb%94%b0%eb%a5%b8-%ec%95%84%ed%82%a4%ed%85%8d%ec%b2%98 aria-label="HA 방식에 따른 아키텍처">HA 방식에 따른 아키텍처</a></li><li><a href=#%ec%84%b1%eb%8a%a5-%ed%85%8c%ec%8a%a4%ed%8a%b8 aria-label="성능 테스트">성능 테스트</a><ul><ul><li><a href=#data-load aria-label="data load">data load</a></li><li><a href=#index-%ec%83%9d%ec%84%b1 aria-label="index 생성">index 생성</a></li><li><a href=#select aria-label=select>select</a></li></ul></ul></li><li><a href=#%ea%b5%ac%ec%84%b1-%eb%b0%a9%eb%b2%95 aria-label="구성 방법">구성 방법</a><ul><ul><li><a href=#%eb%aa%a8%eb%93%a0-%eb%85%b8%eb%93%9c%ec%97%90-citus-%ec%84%a4%ec%b9%98 aria-label="모든 노드에 citus 설치">모든 노드에 citus 설치</a></li><li><a href=#pg_hbaconf-%ec%97%90-%eb%85%b8%eb%93%9c-%ec%b6%94%ea%b0%80 aria-label="pg_hba.conf 에 노드 추가">pg_hba.conf 에 노드 추가</a></li><li><a href=#coordinator-node-%ec%97%90-%ec%9b%8c%ec%bb%a4%eb%85%b8%eb%93%9c-%ec%b6%94%ea%b0%80%ed%95%98%ea%b8%b0 aria-label="coordinator node 에 워커노드 추가하기">coordinator node 에 워커노드 추가하기</a></li><li><a href=#%ec%b6%94%ea%b0%80%eb%90%9c-%ec%9b%8c%ec%bb%a4%eb%85%b8%eb%93%9c-%ed%99%95%ec%9d%b8 aria-label="추가된 워커노드 확인">추가된 워커노드 확인</a></li><li><a href=#distributed-table-%ec%83%9d%ec%84%b1 aria-label="distributed table 생성">distributed table 생성</a></li><li><a href=#coordinator aria-label=coordinator>coordinator</a></li></ul></ul></li><li><a href=#query-processiong-%ea%b3%bc%ec%a0%95 aria-label="Query processiong 과정">Query processiong 과정</a></li><li><a href=#distributed-deadlock aria-label="Distributed Deadlock">Distributed Deadlock</a><ul><ul><li><a href=#coordinator-1 aria-label=coordinator>coordinator</a></li><li><a href=#worker-node-%ea%b4%80%ec%a0%90 aria-label="worker node 관점">worker node 관점</a></li><li><a href=#coordinator-%ea%b4%80%ec%a0%90 aria-label="coordinator 관점">coordinator 관점</a></li></ul></ul></li><li><a href=#%ec%bf%bc%eb%a6%ac-%ec%8b%a4%ed%96%89%ea%b3%84%ed%9a%8d aria-label="쿼리 실행계획">쿼리 실행계획</a><ul><ul><li><a href=#distributed-table-%ea%b0%84-join aria-label="distributed table 간 JOIN">distributed table 간 JOIN</a></li><li><a href=#distributed-table-%ea%b3%bc-local-table-%ea%b0%84-join aria-label="distributed table 과 local table 간 JOIN">distributed table 과 local table 간 JOIN</a></li></ul><li><a href=#%ec%b0%b8%ea%b3%a0-%ec%9e%90%eb%a3%8c aria-label="참고 자료">참고 자료</a></li></ul></li></ul></div></details></div><div class=post-content><hr><p>2년전에 citus가 어떤 것인지 궁금해서 구성 관련 간단히 테스트했던 내용입니다.<br>깊이있는 내용은 전혀 아니고 지금 다시 읽어보니 거의 저만 이해할 수 있게 써놨네요…..<br>현재는 citus나 postgresql 버전이 테스트 당시와 상이할 수 있어 틀린 내용이 많을 수 있습니다.</p><h1 id=목차>목차<a hidden class=anchor aria-hidden=true href=#목차>#</a></h1><ul><li>Citus 란?</li><li>HA 방식에 따른 아키텍처</li><li>성능 테스트</li><li>Citus 설치 방법</li><li>Query processing</li><li>Distributed deadlock</li><li>Query plan</li><li>참고 내용</li></ul><h1 id=citus>Citus<a hidden class=anchor aria-hidden=true href=#citus>#</a></h1><p>-. PostgreSQL 의 scale-out 용도 extension ( mysql plugin )<br>-. hash 기반으로 테이블을 분산함<br>-. 엔터프라이즈 버전은 자동 리밸런싱 + 커넥션 풀 지원<br>-. node 간 HA 별도 구축 필요 -. 별도의 백업정책 필요함<br>-. mongodb sharding + mysql spider engine 아키텍처와 유사</p><p><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/psql/psql_citus_1.png alt></p><pre><code>coordinator : distributed table에 대한 meta data 관리, 클라이언트의 요청을 워커노드에 전달하는 역할 
    * query 최종 sorting 작업 수행 
    
worker node : coordinator로 부터 요청을 받아 실제 데이터를 처리한 후 coordinator 에게 전달하는 역할
    * dml, ddl, analyze, vacuum 등 coordinator 로 부터 전달 받은 커맨드 실행
</code></pre><h1 id=ha-방식에-따른-아키텍처>HA 방식에 따른 아키텍처<a hidden class=anchor aria-hidden=true href=#ha-방식에-따른-아키텍처>#</a></h1><ul><li><p>shard_replication_factor<br><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/psql/psql_citus_2.png alt><br>=> worker node down 에 대비하여 분산 테이블을 다른 worker node에도 복제하는 방식<br>구축은 쉬우나 worker node down 시 일부 dml 불가 현상 및 수동으로 sync 맞춰줘야 하는 단점이 있음</p></li><li><p>replication + pgpool+master_add_secondary_node<br><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/psql/psql_citus_3.png alt><br>=> Primary node down 시 auto-failover 된 standby node가 투입되는 방식</p></li><li><p>streaming replication : postgresql 의 복제 방식으로 mysql처럼 sql thread /io thread 방식과 유사함 (PostgreSQL에서는 sender / receiver, sql 커맨드 대신 트랜잭션로그)</p></li><li><p>pgpool : auto - failover</p></li></ul><h1 id=성능-테스트>성능 테스트<a hidden class=anchor aria-hidden=true href=#성능-테스트>#</a></h1><p><strong>single node와 citus - 3 worker nodes 아키텍처 (azure) 간 성능 테스트</strong></p><h3 id=data-load>data load<a hidden class=anchor aria-hidden=true href=#data-load>#</a></h3><ul><li>single</li></ul><pre><code>citus=&gt; \copy github_events FROM large_events.csv with csv
COPY 1146625
Time: 1451256.144 ms (24:11.256)
</code></pre><ul><li>citus</li></ul><pre><code>citus=&gt; \copy github_events from 'large_events.csv' with csv;
COPY 1146625
Time: 879733.291 ms (14:39.733)
</code></pre><h3 id=index-생성>index 생성<a hidden class=anchor aria-hidden=true href=#index-생성>#</a></h3><ul><li>single</li></ul><pre><code>citus=&gt; create index test_idx on github_events using gin(payload jsonb_path_ops);
CREATE INDEX
Time: 193734.521 ms (03:13.735)
</code></pre><ul><li>citus</li></ul><pre><code>citus=&gt; create index test_idx on github_events using gin(payload jsonb_path_ops);
CREATE INDEX
Time: 49713.177 ms (00:49.713)
</code></pre><h3 id=select>select<a hidden class=anchor aria-hidden=true href=#select>#</a></h3><ul><li>single</li></ul><pre><code>SELECT date_trunc('minute', created_at) AS minute,
       sum((payload-&gt;&gt;'distinct_size')::int) AS num_commits
FROM github_events
WHERE event_type = 'PushEvent'
GROUP BY minute
ORDER BY minute;

Time: 1213.770 ms (00:01.214)
</code></pre><ul><li>citus</li></ul><pre><code>SELECT date_trunc('minute', created_at) AS minute,
       sum((payload-&gt;&gt;'distinct_size')::int) AS num_commits
FROM github_events
WHERE event_type = 'PushEvent'
GROUP BY minute
ORDER BY minute;

Time: 847.792 ms
</code></pre><h1 id=구성-방법>구성 방법<a hidden class=anchor aria-hidden=true href=#구성-방법>#</a></h1><h3 id=모든-노드에-citus-설치>모든 노드에 citus 설치<a hidden class=anchor aria-hidden=true href=#모든-노드에-citus-설치>#</a></h3><pre><code>docker network create citus-test
docker run --name citus_master -d --network citus-test citusdata/citus
docker run --name citus_slave  -d --network citus-test citusdata/citus
docker run --name citus_worker1 -d  --network citus-test citusdata/citus
docker run --name citus_worker2 -d  --network citus-test citusdata/citus
docker run --name citus_worker3 -d  --network citus-test citusdata/citus

docker run --name citus_worker1_standby -d  --network citus-test citusdata/citus
docker run --name citus_worker2_standby -d  --network citus-test citusdata/citus
docker run --name citus_worker3_standby -d  --network citus-test citusdata/citus
</code></pre><p>=> primary worker node 3대 + standby worker node 3대 + coordinator M/S 하나씩<br>streaming replication + pgpool 로 HA 구성</p><pre><code>postgres=# select * from pg_available_extensions where name='citus';
 name  | default_version | installed_version |          comment
-------+-----------------+-------------------+----------------------------
 citus | 8.3-1           | 8.3-1             | Citus distributed database
(1 row)
</code></pre><p>=> citus extension 설치 (extension 은 mysql 의 plugin 개념)</p><h3 id=pg_hbaconf-에-노드-추가>pg_hba.conf 에 노드 추가<a hidden class=anchor aria-hidden=true href=#pg_hbaconf-에-노드-추가>#</a></h3><pre><code>postgres@07a3ccbd0898:~/data$ tail -5f pg_hba.conf
host    replication     all             172.19.0.0/24           trust
host    all             all             172.19.0.0/24           trust

postgres@07a3ccbd0898:~$ /usr/lib/postgresql/11/bin/pg_ctl reload -D /var/lib/postgresql/data
server signaled
</code></pre><p>=> oracle 의 sqlnet.ora, cubrid iplist , mysql ip/host 개념처럼 해당 db에 붙을 수 있는 host 허용하는 부분</p><h3 id=coordinator-node-에-워커노드-추가하기>coordinator node 에 워커노드 추가하기<a hidden class=anchor aria-hidden=true href=#coordinator-node-에-워커노드-추가하기>#</a></h3><pre><code>postgres=# SELECT master_add_node ('citus_worker1',5432);
                   master_add_node
------------------------------------------------------
 (7,7,citus_worker1,5432,default,f,t,primary,default)
(1 row)

postgres=# SELECT master_add_node ('citus_worker2',5432);
                   master_add_node
------------------------------------------------------
 (8,8,citus_worker2,5432,default,f,t,primary,default)
(1 row)

postgres=# SELECT master_add_node ('citus_worker3',5432);
                   master_add_node
------------------------------------------------------
 (9,9,citus_worker3,5432,default,f,t,primary,default)
(1 row)


postgres=# select master_add_secondary_node('citus_worker1_standby','5432','citus_worker1','5432');
                    master_add_secondary_node
-----------------------------------------------------------------
 (10,7,citus_worker1_standby,5432,default,f,t,secondary,default)
(1 row)

postgres=# select master_add_secondary_node('citus_worker2_standby','5432','citus_worker2','5432');
                    master_add_secondary_node
-----------------------------------------------------------------
 (10,7,citus_worker2_standby,5432,default,f,t,secondary,default)
(1 row)

postgres=# select master_add_secondary_node('citus_worker3_standby','5432','citus_worker3','5432');
                    master_add_secondary_node
-----------------------------------------------------------------
 (10,7,citus_worker3_standby,5432,default,f,t,secondary,default)
(1 row)
</code></pre><p>=> hostname,port 로 worker node, standby worker node 추가</p><h3 id=추가된-워커노드-확인>추가된 워커노드 확인<a hidden class=anchor aria-hidden=true href=#추가된-워커노드-확인>#</a></h3><pre><code>postgres=# select * from pg_dist_node;
 nodeid | groupid |       nodename        | nodeport | noderack | hasmetadata | isactive | noderole  | nodecluster
--------+---------+-----------------------+----------+----------+-------------+----------+-----------+-------------
      7 |       7 | citus_worker1         |     5432 | default  | f           | t        | primary   | default
      8 |       8 | citus_worker2         |     5432 | default  | f           | t        | primary   | default
      9 |       9 | citus_worker3         |     5432 | default  | f           | t        | primary   | default
     10 |       7 | citus_worker1_standby |     5432 | default  | f           | t        | secondary | default
     11 |       8 | citus_worker2_standby |     5432 | default  | f           | t        | secondary | default
     12 |       9 | citus_worker3_standby |     5432 | default  | f           | t        | secondary | default
(6 rows)
</code></pre><h3 id=distributed-table-생성>distributed table 생성<a hidden class=anchor aria-hidden=true href=#distributed-table-생성>#</a></h3><ul><li>citus coordinator 에서 수행</li></ul><pre><code>postgres=# set citus.shard_count=6;
SET

# shard_replication_factor 사용할 땐 아래 설정 추가
postgres=# set citus.shard_replication_factor=2;
SET
</code></pre><ul><li>citus.shard_count : 해당 테이블을 몇개로 쪼갤 것인지 설정 (default 32)</li><li>citus.shard_replication_factor : 쪼개진 테이블의 복사본을 몇개 갖고 있을 것인지 설정 (default 1)</li><li>( shard_count 6 * replication_factor 2 ) / worker node 3 = 4 => 한 노드에 쪼개진 테이블 4개씩 갖고있게됨</li></ul><pre><code>CREATE TABLE companies (
    id bigint NOT NULL,
    name text NOT NULL,
    image_url text,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone NOT NULL
);

CREATE TABLE campaigns (
    id bigint NOT NULL,
    company_id bigint NOT NULL,
    name text NOT NULL,
    cost_model text NOT NULL,
    state text NOT NULL,
    monthly_budget bigint,
    blacklisted_site_urls text[],
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone NOT NULL
);

CREATE TABLE ads (
    id bigint NOT NULL,
    company_id bigint NOT NULL,
    campaign_id bigint NOT NULL,
    name text NOT NULL,
    image_url text,
    target_url text,
    impressions_count bigint DEFAULT 0,
    clicks_count bigint DEFAULT 0,
    created_at timestamp without time zone NOT NULL,
    updated_at timestamp without time zone NOT NULL
);
</code></pre><p>=> test 용 테이블 생성</p><pre><code>SELECT create_distributed_table('companies', 'id');
SELECT create_distributed_table('campaigns', 'company_id');
SELECT create_distributed_table('ads', 'company_id');
</code></pre><p>=> 생성한 테이블을 각각 id, company_id, company_id 기준으로 distributed table 생성</p><h3 id=coordinator>coordinator<a hidden class=anchor aria-hidden=true href=#coordinator>#</a></h3><pre><code>postgres=# \dt
              List of relations
 Schema |       Name       | Type  |  Owner
--------+------------------+-------+----------
 public | ads              | table | postgres
 public | campaigns        | table | postgres
 public | companies        | table | postgres
</code></pre><ul><li>worker node 1</li></ul><pre><code>public | campaigns_102174        | table | postgres
public | campaigns_102176        | table | postgres
public | campaigns_102178        | table | postgres
</code></pre><ul><li>worker node 2</li></ul><pre><code>public | campaigns_102175        | table | postgres
public | campaigns_102177        | table | postgres
public | campaigns_102179        | table | postgres
</code></pre><pre><code>postgres=# select a.logicalrelid,a.shardid,a.shardminvalue,a.shardmaxvalue,b.shardstate,b.groupid
from pg_dist_shard a inner join pg_dist_placement b on a.shardid=b.shardid
where a.logicalrelid=CAST('campaigns' as regclass);

 logicalrelid | shardid | shardminvalue | shardmaxvalue | shardstate | groupid
--------------+---------+---------------+---------------+------------+---------
 campaigns    |  102174 | -2147483648   | -1431655767   |          1 |       1
 campaigns    |  102175 | -1431655766   | -715827885    |          1 |       2
 campaigns    |  102176 | -715827884    | -3            |          1 |       1
 campaigns    |  102177 | -2            | 715827879     |          1 |       2
 campaigns    |  102178 | 715827880     | 1431655761    |          1 |       1
 campaigns    |  102179 | 1431655762    | 2147483647    |          1 |       2
</code></pre><p>=> 생성된 distributed table 과 매핑되는 hash value range</p><h1 id=query-processiong-과정>Query processiong 과정<a hidden class=anchor aria-hidden=true href=#query-processiong-과정>#</a></h1><p><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/psql/psql_citus_4.png alt></p><pre><code>postgres=# SELECT master_get_table_metadata('test');
  master_get_table_metadata
------------------------------
 (16759,t,h,t,1,1073741824,2)
(1 row)

postgres=#select * From pg_backend_pid();
 pg_backend_pid
----------------
            796
(1 row)

postgres=# select * from test;
</code></pre><p>=> 디버깅 대상 세션 준비</p><pre><code>[root@1a99d5cd2d41 /]# gdb -p 796
(gdb) b CreateDistributedPlan
Breakpoint 1 at 0x7fc3a0bf9e20: file planner/distributed_planner.c, line 575.
(gdb) b ExecProcNode
Breakpoint 2 at 0x5f3225: ExecProcNode. (55 locations)
(gdb) b IsDistributedTable
Breakpoint 3 at 0x7fc3a0c25560: file utils/metadata_cache.c, line 269.
(gdb) b CitusExecutorRun
Breakpoint 4 at 0x7fc3a0be9580: file executor/multi_executor.c, line 108.
(gdb) b CitusExecutorStart
Breakpoint 5 at 0x7fc3a0be9480: file executor/multi_executor.c, line 64.
</code></pre><p>=> gdb breakpoint 설정</p><pre><code>Breakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269
269	{
(gdb) bt
#0  IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269
#1  0x00007fc3a0bf9963 in ListContainsDistributedTableRTE (rangeTableList=rangeTableList@entry=0x1961150)
    at planner/distributed_planner.c:289
#2  0x00007fc3a0bfa525 in distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0)
    at planner/distributed_planner.c:117
#3  0x00000000007261fc in pg_plan_query (querytree=querytree@entry=0x1960a68, cursorOptions=cursorOptions@entry=256,
    boundParams=boundParams@entry=0x0) at postgres.c:832
#4  0x00000000007262de in pg_plan_queries (querytrees=&lt;optimized out&gt;, cursorOptions=cursorOptions@entry=256,
    boundParams=boundParams@entry=0x0) at postgres.c:898
#5  0x000000000072674a in exec_simple_query (query_string=0x195fc10 &quot;select * from test;&quot;) at postgres.c:1073
#6  0x00000000007278c2 in PostgresMain (argc=&lt;optimized out&gt;, argv=argv@entry=0x19b2480, dbname=0x19b2328 &quot;postgres&quot;,
    username=&lt;optimized out&gt;) at postgres.c:4182
#7  0x000000000047b0cf in BackendRun (port=0x19aa6c0) at postmaster.c:4358
#8  BackendStartup (port=0x19aa6c0) at postmaster.c:4030
#9  ServerLoop () at postmaster.c:1707
#10 0x00000000006be989 in PostmasterMain (argc=argc@entry=3, argv=argv@entry=0x195b960) at postmaster.c:1380
#11 0x000000000047bb11 in main (argc=3, argv=0x195b960) at main.c:228
</code></pre><p>=> “select * from test” parsing 후 distributed_planner 에서 해당 테이블이 distributed table인지 check ((relationId=16759)</p><pre><code>Breakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269
269	{
(gdb) bt
#0  CreateDistributedPlan (planId=planId@entry=5, originalQuery=originalQuery@entry=0x1960958, query=query@entry=0x1960a68,
    boundParams=boundParams@entry=0x0, hasUnresolvedParams=hasUnresolvedParams@entry=false,
    plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/distributed_planner.c:575
#1  0x00007fc3a0bfa34d in CreateDistributedPlannedStmt (plannerRestrictionContext=0x1961608, boundParams=0x0, query=0x1960a68,
    originalQuery=0x1960958, localPlan=0x19af2c0, planId=5) at planner/distributed_planner.c:493
#2  distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0) at planner/distributed_planner.c:185
</code></pre><p>=> distributed plan 생성 시작</p><pre><code>Breakpoint 3, IsDistributedTable (relationId=16759) at utils/metadata_cache.c:269
269	{
(gdb) bt
#0  IsDistributedTable (relationId=relationId@entry=16759) at utils/metadata_cache.c:269
#1  0x00007fc3a0c0a771 in MultiRouterPlannableQuery (query=query@entry=0x1960a68) at planner/multi_router_planner.c:3002
#2  0x00007fc3a0c0cc61 in CreateRouterPlan (originalQuery=originalQuery@entry=0x1960958, query=query@entry=0x1960a68,
    plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/multi_router_planner.c:179
#3  0x00007fc3a0bf9ebe in CreateDistributedPlan (planId=planId@entry=5, originalQuery=originalQuery@entry=0x1960958,
    query=query@entry=0x1960a68, boundParams=boundParams@entry=0x0, hasUnresolvedParams=hasUnresolvedParams@entry=false,
    plannerRestrictionContext=plannerRestrictionContext@entry=0x1961608) at planner/distributed_planner.c:632
#4  0x00007fc3a0bfa34d in CreateDistributedPlannedStmt (plannerRestrictionContext=0x1961608, boundParams=0x0, query=0x1960a68,
    originalQuery=0x1960958, localPlan=0x19af2c0, planId=5) at planner/distributed_planner.c:493
#5  distributed_planner (parse=0x1960a68, cursorOptions=256, boundParams=0x0) at planner/distributed_planner.c:185
</code></pre><p>=> 어떤 worker node로 보낼지 plan 생성</p><pre><code>Breakpoint 5, CitusExecutorStart (queryDesc=0x1a78a20, eflags=0) at executor/multi_executor.c:64
64	{
(gdb) bt
#0  CitusExecutorStart (queryDesc=0x1a78a20, eflags=0) at executor/multi_executor.c:64
</code></pre><p>=> 생성된 distributed plan 을 worker node에서 실행시킬 준비</p><pre><code>Breakpoint 4, CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0, execute_once=true)
    at executor/multi_executor.c:108
108	{
(gdb) bt
#0  CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0, execute_once=true)
    at executor/multi_executor.c:108


Breakpoint 2, ExecutePlan (execute_once=&lt;optimized out&gt;, dest=0x1b0d4d0, direction=ForwardScanDirection, numberTuples=0,
    sendTuples=true, operation=CMD_SELECT, use_parallel_mode=&lt;optimized out&gt;, planstate=0x1a717c0, estate=0x1a715b0)
    at execMain.c:1723
1723			slot = ExecProcNode(planstate);
(gdb) bt
#0  ExecutePlan (execute_once=&lt;optimized out&gt;, dest=0x1b0d4d0, direction=ForwardScanDirection, numberTuples=0, sendTuples=true,
    operation=CMD_SELECT, use_parallel_mode=&lt;optimized out&gt;, planstate=0x1a717c0, estate=0x1a715b0) at execMain.c:1723
#1  standard_ExecutorRun (queryDesc=queryDesc@entry=0x1a78a20, direction=direction@entry=ForwardScanDirection,
    count=count@entry=0, execute_once=execute_once@entry=true) at execMain.c:364
#2  0x00007fc3a0be95d3 in CitusExecutorRun (queryDesc=0x1a78a20, direction=ForwardScanDirection, count=0,
    execute_once=&lt;optimized out&gt;) at executor/multi_executor.c:151
</code></pre><p>=> 각각의 worker node 에서 query 수행</p><h1 id=distributed-deadlock>Distributed Deadlock<a hidden class=anchor aria-hidden=true href=#distributed-deadlock>#</a></h1><h3 id=coordinator-1>coordinator<a hidden class=anchor aria-hidden=true href=#coordinator-1>#</a></h3><pre><code>| session 1 | session 2  |
| --- | --- |
|  update test set b='session1' where a=1;|   |
|  | update test set b='session2' where a=2; |
| update test set b='session1' where a=2; |  |
|  | update test set b='session2' where a=1; |
</code></pre><h3 id=worker-node-관점>worker node 관점<a hidden class=anchor aria-hidden=true href=#worker-node-관점>#</a></h3><pre><code>| worker1 |  | worker2  |  |
| --- | --- | --- | --- |
| update test_worker1 set b='session1' where a=1; | holder  | update test_worker2 set b='session2' where a=2; | holder |
| update test_worker1 set b='session2' where a=1; | waiter  | update test_worker2 set b='session1' where a=2;  | waiter |
</code></pre><p>=> 각 worker node가 봤을 때는 단순한 lock 대기상황</p><h3 id=coordinator-관점>coordinator 관점<a hidden class=anchor aria-hidden=true href=#coordinator-관점>#</a></h3><p>=> session1의 트랜잭션은 worker node2 에서 session2 에 의해 대기,<br>session2 의 트랜잭션은 worker node1 에서 session1 에 의해 대기하는 deadlock 상황</p><pre><code>* session 1 (1,1)
(gdb) info locals
currentDistributedTransactionId = 0x10b5340
backendData = {databaseId = 12368, userId = 10, mutex = 1 '\001', cancelledDueToDeadlock = false, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true},
  transactionId = {initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 16, timestamp = 622787261936074}}


* session 2 (2,2)
currentDistributedTransactionId = 0x10c0340
backendData = {databaseId = 12368, userId = 10, mutex = 1 '\001', cancelledDueToDeadlock = true, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = {
    initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 17, timestamp = 622787449815181}}

* session 1 (1,2)
currentDistributedTransactionId = 0x10b5340
backendData = {databaseId = 12368, userId = 10, mutex = 1 '\001', cancelledDueToDeadlock = false, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true},
  transactionId = {initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 16, timestamp = 622787261936074}}


* session 2 (2,1)
(gdb) info locals
currentDistributedTransactionId = 0x10c0340
backendData = {databaseId = 12368, userId = 10, mutex = 1 '\001', cancelledDueToDeadlock = true, citusBackend = {initiatorNodeIdentifier = 0, transactionOriginator = true}, transactionId = {
    initiatorNodeIdentifier = 0, transactionOriginator = true, transactionNumber = 17, timestamp = 622787449815181}}
</code></pre><p>=> deadlock 발생구간에서 트랜잭션ID 가 큰 후순위 트랜잭션을 kill 해서 deadlock 처리함</p><h1 id=쿼리-실행계획>쿼리 실행계획<a hidden class=anchor aria-hidden=true href=#쿼리-실행계획>#</a></h1><h3 id=distributed-table-간-join>distributed table 간 JOIN<a hidden class=anchor aria-hidden=true href=#distributed-table-간-join>#</a></h3><pre><code>set citus.explain_all_tasks = on ;

SELECT campaigns.id, campaigns.name, campaigns.monthly_budget,
       sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks
FROM ads, campaigns
WHERE ads.company_id = campaigns.company_id
AND campaigns.company_id=5
AND campaigns.state = 'running'
GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget
ORDER BY total_impressions, total_clicks;


                                                  QUERY PLAN
---------------------------------------------------------------------------------------------------------------
 Custom Scan (Citus Adaptive)  (cost=0.00..0.00 rows=0 width=0)
   Task Count: 1
   Tasks Shown: All
   -&gt;  Task
         Node: host=citus_worker3 port=5432 dbname=postgres
         -&gt;  Sort  (cost=49.87..49.88 rows=3 width=94)
               Sort Key: (sum(ads.impressions_count)), (sum(ads.clicks_count))
               -&gt;  HashAggregate  (cost=49.81..49.85 rows=3 width=94)
                     Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                     -&gt;  Nested Loop  (cost=0.00..46.54 rows=261 width=46)
                           -&gt;  Seq Scan on ads_102021 ads  (cost=0.00..38.04 rows=87 width=24)
                                 Filter: (company_id = 5)
                           -&gt;  Materialize  (cost=0.00..5.25 rows=3 width=38)
                                 -&gt;  Seq Scan on campaigns_102015 campaigns  (cost=0.00..5.23 rows=3 width=38)
                                       Filter: ((company_id = 5) AND (state = 'running'::text))
</code></pre><ul><li>두 테이블 간 조인을 하는데 citus_worker3 한 노드에서만 데이터를 가져옴</li><li>동일한 distributed column ( company_id =5 )에 대해서는 어차피 해쉬값이 같기 때문에 한노드에 저장됨</li><li>명시적으로는 아래와 같이 설정 가능<ul><li>SELECT create_distributed_table(‘event’, ‘tenant_id’);</li><li>SELECT create_distributed_table(‘page’, ‘tenant_id’, colocate_with => ‘event’);</li></ul></li></ul><pre><code>SELECT campaigns.id, campaigns.name, campaigns.monthly_budget,
       sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks
FROM ads, campaigns
WHERE ads.company_id = campaigns.company_id
AND campaigns.company_id between 1 and 100
AND campaigns.state = 'running'
GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget
ORDER BY total_impressions, total_clicks;
                                                                   QUERY PLAN
    -----------------------------------------------------------------------------------------------------------------------------------------
     Sort  (cost=0.00..0.00 rows=0 width=0)
       Sort Key: pg_catalog.sum((pg_catalog.sum(remote_scan.total_impressions))), pg_catalog.sum((pg_catalog.sum(remote_scan.total_clicks)))
       -&gt;  HashAggregate  (cost=0.00..0.00 rows=0 width=0)
             Group Key: remote_scan.id, remote_scan.name, remote_scan.monthly_budget
             -&gt;  Custom Scan (Citus Adaptive)  (cost=0.00..0.00 rows=0 width=0)
                   Task Count: 6
                   Tasks Shown: All
                   -&gt;  Task
                         Node: host=citus_worker1 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=185.41..186.35 rows=63 width=94)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=58.58..123.37 rows=4963 width=46)
                                     Hash Cond: (campaigns.company_id = ads.company_id)
                                     -&gt;  Seq Scan on campaigns_102014 campaigns  (cost=0.00..6.19 rows=63 width=38)
                                           Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                                     -&gt;  Hash  (cost=41.59..41.59 rows=1359 width=24)
                                           -&gt;  Seq Scan on ads_102020 ads  (cost=0.00..41.59 rows=1359 width=24)
                   -&gt;  Task
                         Node: host=citus_worker2 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=136.52..137.12 rows=40 width=94)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=49.27..94.11 rows=3393 width=46)
                                     Hash Cond: (campaigns.company_id = ads.company_id)
                                     -&gt;  Seq Scan on campaigns_102015 campaigns  (cost=0.00..5.61 rows=40 width=38)
                                           Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                                     -&gt;  Hash  (cost=35.23..35.23 rows=1123 width=24)
                                           -&gt;  Seq Scan on ads_102021 ads  (cost=0.00..35.23 rows=1123 width=24)
                   -&gt;  Task
                         Node: host=citus_worker3 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=161.65..162.44 rows=53 width=94)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=49.67..106.97 rows=4374 width=46)
                                     Hash Cond: (campaigns.company_id = ads.company_id)
                                     -&gt;  Seq Scan on campaigns_102016 campaigns  (cost=0.00..5.68 rows=53 width=38)
                                           Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                                     -&gt;  Hash  (cost=35.41..35.41 rows=1141 width=24)
                                           -&gt;  Seq Scan on ads_102022 ads  (cost=0.00..35.41 rows=1141 width=24)
                   -&gt;  Task
                         Node: host=citus_worker1 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=156.06..156.83 rows=51 width=94)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=47.91..103.41 rows=4212 width=46)
                                     Hash Cond: (campaigns.company_id = ads.company_id)
                                     -&gt;  Seq Scan on campaigns_102017 campaigns  (cost=0.00..5.61 rows=51 width=38)
                                           Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                                     -&gt;  Hash  (cost=34.07..34.07 rows=1107 width=24)
                                           -&gt;  Seq Scan on ads_102023 ads  (cost=0.00..34.07 rows=1107 width=24)
                   -&gt;  Task
                         Node: host=citus_worker2 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=161.72..162.56 rows=56 width=95)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=6.13..103.98 rows=4619 width=47)
                                     Hash Cond: (ads.company_id = campaigns.company_id)
                                     -&gt;  Seq Scan on ads_102024 ads  (cost=0.00..31.22 rows=1022 width=24)
                                     -&gt;  Hash  (cost=5.43..5.43 rows=56 width=39)
                                           -&gt;  Seq Scan on campaigns_102018 campaigns  (cost=0.00..5.43 rows=56 width=39)
                                                 Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                   -&gt;  Task
                         Node: host=citus_worker3 port=5432 dbname=postgres
                         -&gt;  HashAggregate  (cost=215.04..216.08 rows=69 width=94)
                               Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                               -&gt;  Hash Join  (cost=70.27..144.78 rows=5621 width=46)
                                     Hash Cond: (campaigns.company_id = ads.company_id)
                                     -&gt;  Seq Scan on campaigns_102019 campaigns  (cost=0.00..7.61 rows=69 width=38)
                                           Filter: ((company_id &gt;= 1) AND (company_id &lt;= 100) AND (state = 'running'::text))
                                     -&gt;  Hash  (cost=50.12..50.12 rows=1612 width=24)
                                           -&gt;  Seq Scan on ads_102025 ads  (cost=0.00..50.12 rows=1612 width=24)
</code></pre><ul><li>company_id 조건을 between 범위검색으로 변경</li><li>company_id =5 일 때 target query 로 처리되었던 것과는 달리 broad cast 로 모든 worker node 사용하게 됨</li><li>각각의 worker_node 에서 데이터 추출 후 최종적으로 coordinator에서 취합 & sorting 수행</li><li>mongodb 에서는 shard node 간 primary 선출, sorting 후 mongus 로 return 하는 것과는 달리 coordinator 에서 더 많은 역할을 수행함</li></ul><h3 id=distributed-table-과-local-table-간-join>distributed table 과 local table 간 JOIN<a hidden class=anchor aria-hidden=true href=#distributed-table-과-local-table-간-join>#</a></h3><pre><code>SELECT campaigns.id, campaigns.name, campaigns.monthly_budget,
       sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks
FROM local_ads ads, campaigns
WHERE ads.company_id = campaigns.company_id
AND campaigns.company_id=5
AND campaigns.state = 'running'
GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget
ORDER BY total_impressions, total_clicks;

ERROR:  relation local_ads is not distributed
</code></pre><p>=> local table , 분산되지 않은 일반 테이블은 distributed table 과 조인이 불가능함.<br>아래와 같이 inline view 로 풀어서 수행</p><pre><code>SELECT campaigns.id, campaigns.name, campaigns.monthly_budget,
       sum(impressions_count) as total_impressions, sum(clicks_count) as total_clicks
FROM (select * from local_ads) ads, campaigns
WHERE ads.company_id = campaigns.company_id
AND campaigns.company_id=5
AND campaigns.state = 'running'
GROUP BY campaigns.id, campaigns.name, campaigns.monthly_budget
ORDER BY total_impressions, total_clicks;

                                                           QUERY PLAN
---------------------------------------------------------------------------------------------------------------------------------
 Custom Scan (Citus Adaptive)  (cost=0.00..0.00 rows=0 width=0)
   -&gt;  Distributed Subplan 3_1
         -&gt;  Seq Scan on local_ads  (cost=0.00..225.64 rows=7364 width=134)
   Task Count: 1
   Tasks Shown: All
   -&gt;  Task
         Node: host=citus_worker2 port=5432 dbname=postgres
         -&gt;  Sort  (cost=18.19..18.20 rows=3 width=94)
               Sort Key: (sum(intermediate_result.impressions_count)), (sum(intermediate_result.clicks_count))
               -&gt;  HashAggregate  (cost=18.12..18.17 rows=3 width=94)
                     Group Key: campaigns.id, campaigns.name, campaigns.monthly_budget
                     -&gt;  Nested Loop  (cost=0.00..17.93 rows=15 width=46)
                           -&gt;  Function Scan on read_intermediate_result intermediate_result  (cost=0.00..12.50 rows=5 width=24)
                                 Filter: (company_id = 5)
                           -&gt;  Materialize  (cost=0.00..5.25 rows=3 width=38)
                                 -&gt;  Seq Scan on campaigns_102015 campaigns  (cost=0.00..5.23 rows=3 width=38)
                                       Filter: ((company_id = 5) AND (state = 'running'::text))
</code></pre><h2 id=참고-자료>참고 자료<a hidden class=anchor aria-hidden=true href=#참고-자료>#</a></h2><ul><li><a href=https://docs.citusdata.com/en/v8.0/index.html>https://docs.citusdata.com/en/v8.0/index.html</a></li><li><a href="https://www.youtube.com/watch?v=MwMNZxr-IZM&t=1607s">https://www.youtube.com/watch?v=MwMNZxr-IZM&t=1607s</a></li><li><a href="https://browndwarf.tistory.com/7?category=803646">https://browndwarf.tistory.com/7?category=803646</a></li><li><a href=https://github.com/citusdata/citus>https://github.com/citusdata/citus</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=/tags/postgresql/>postgresql</a></li><li><a href=/tags/citus/>citus</a></li><li><a href=/tags/sharding/>sharding</a></li><li><a href=/tags/architecture/>architecture</a></li></ul></footer><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")
return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='kimdubia';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href>kimDuBiA</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script><script>document.querySelectorAll('pre > code').forEach((codeblock)=>{const container=codeblock.parentNode.parentNode;const copybutton=document.createElement('button');copybutton.classList.add('copy-code');copybutton.innerText='copy';function copyingDone(){copybutton.innerText='copied!';setTimeout(()=>{copybutton.innerText='copy';},2000);}
copybutton.addEventListener('click',(cb)=>{if('clipboard'in navigator){navigator.clipboard.writeText(codeblock.textContent);copyingDone();return;}
const range=document.createRange();range.selectNodeContents(codeblock);const selection=window.getSelection();selection.removeAllRanges();selection.addRange(range);try{document.execCommand('copy');copyingDone();}catch(e){};selection.removeRange(range);});if(container.classList.contains("highlight")){container.appendChild(copybutton);}else if(container.parentNode.firstChild==container){}else if(codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"){codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);}else{codeblock.parentNode.appendChild(copybutton);}});</script></body></html>