<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus) | kimDuBiA</title><meta name=keywords content="kubernetes,redis"><meta name=description content="Prometheus Operator? 개념  Operator : Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함 ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함  kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,
Operator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음
모니터링을 구축해보자 기존 redis-cluster helm chart 수정  helm values.yaml 수정  ## Prometheus Exporter / Metrics metrics: enabled: true # Enable this if you're using https://github."><meta name=author content="kimdubi"><link rel=canonical href=/cloud/k8s_redis_cluster_monitoring/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.min.2dbef8664bbfb3e83a0a44fd4a8fc5010240dbcd1dbc1400753b928b497b8f5e.css integrity="sha256-Lb74Zku/s+g6CkT9So/FAQJA280dvBQAdTuSi0l7j14=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><link rel=icon href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme: rgb(29, 30, 32);--entry: rgb(46, 46, 51);--primary: rgb(218, 218, 219);--secondary: rgb(155, 156, 157);--tertiary: rgb(65, 66, 68);--content: rgb(196, 196, 197);--hljs-bg: rgb(46, 46, 51);--code-bg: rgb(55, 56, 62);--border: rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');ga('create','UA-123-45','auto');ga('send','pageview');}</script><meta property="og:title" content="kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)"><meta property="og:description" content="Prometheus Operator? 개념  Operator : Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함 ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함  kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,
Operator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음
모니터링을 구축해보자 기존 redis-cluster helm chart 수정  helm values.yaml 수정  ## Prometheus Exporter / Metrics metrics: enabled: true # Enable this if you're using https://github."><meta property="og:type" content="article"><meta property="og:url" content="/cloud/k8s_redis_cluster_monitoring/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="cloud"><meta property="article:published_time" content="2021-10-06T21:15:29+09:00"><meta property="article:modified_time" content="2021-10-06T21:15:29+09:00"><meta property="og:site_name" content="kimDuBiA"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)"><meta name=twitter:description content="Prometheus Operator? 개념  Operator : Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함 ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함  kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,
Operator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음
모니터링을 구축해보자 기존 redis-cluster helm chart 수정  helm values.yaml 수정  ## Prometheus Exporter / Metrics metrics: enabled: true # Enable this if you're using https://github."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Clouds","item":"/cloud/"},{"@type":"ListItem","position":3,"name":"kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)","item":"/cloud/k8s_redis_cluster_monitoring/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)","name":"kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)","description":"Prometheus Operator? 개념  Operator : Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함 ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함  kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,\nOperator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음\n모니터링을 구축해보자 기존 redis-cluster helm chart 수정  helm values.yaml 수정  ## Prometheus Exporter / Metrics metrics: enabled: true # Enable this if you're using https://github.","keywords":["kubernetes","redis"],"articleBody":" Prometheus Operator? 개념  Operator : Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함 ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함  kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,\nOperator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음\n모니터링을 구축해보자 기존 redis-cluster helm chart 수정  helm values.yaml 수정  ## Prometheus Exporter / Metrics metrics: enabled: true # Enable this if you're using https://github.com/coreos/prometheus-operator serviceMonitor: enabled: true = redis-cluster Pod에 Prometheus Exporter 를 띄우겠다는 설정\nmetrics와 serviceMonitor를 띄워 metric를 수집하도록 설정함\n redis-exporter 확인  $ kubectl get pod NAME READY STATUS RESTARTS AGE kimdubi-test-redis-cluster-0 2/2 Running 0 3m21s kimdubi-test-redis-cluster-1 2/2 Running 0 3m53s kimdubi-test-redis-cluster-2 2/2 Running 0 4m28s kimdubi-test-redis-cluster-3 2/2 Running 0 5m12s kimdubi-test-redis-cluster-4 2/2 Running 0 5m53s kimdubi-test-redis-cluster-5 2/2 Running 0 6m18s $ kubectl exec -it kimdubi-test-redis-cluster-0 -c metrics /bin/sh kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead. $ ps -ef UID PID PPID C STIME TTY TIME CMD 1001 1 0 0 07:35 ? 00:00:00 redis_exporter = Pod 내 container가 1/1 - 2/2, Pod 하나에 Container 두대가 떴다는 의미\nPod 접속 시 -c 옵션을 통해 Container명을 지정해서 접속할 수 있음\nprometheus-opertor 설치  prometheus-operator 다운로드  $ helm fetch stable/prometheus-operator $ tar zxvpf prometheus-operator-9.3.2.tgz  helm chart 수정  ### vi ~/prometheus-operator/charts/grafana persistence: type: pvc enabled: true storageClassName: kimdubi-test accessModes: - ReadWriteOnce size: 1Gi  helm install  $ helm install --namespace monitoring --create-namespace monitoring ./prometheus-operator  구성 확인  $ helm ls -n monitoring NAME NAMESPACE REVISION\tUPDATED STATUS CHART APP VERSION monitoring\tmonitoring\t1 2021-03-17 16:13:07.047828 +0900 KST\tdeployed\tprometheus-operator-9.3.2\t0.38.1 $ kubectl get all -n monitoring NAME READY STATUS RESTARTS AGE pod/alertmanager-monitoring-prometheus-oper-alertmanager-0 2/2 Running 0 4m13s pod/monitoring-grafana-5694798c88-dhnqf 2/2 Running 0 4m20s pod/monitoring-kube-state-metrics-5f4d9ddc46-qfmrj 1/1 Running 0 4m20s pod/monitoring-prometheus-node-exporter-6vd26 1/1 Running 0 4m20s pod/monitoring-prometheus-node-exporter-r99gf 1/1 Running 0 4m20s pod/monitoring-prometheus-node-exporter-x4qfw 1/1 Running 0 4m20s pod/monitoring-prometheus-oper-operator-8bd4fb5b8-7xr9q 2/2 Running 0 4m20s pod/prometheus-monitoring-prometheus-oper-prometheus-0 3/3 Running 1 4m3s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/alertmanager-operated ClusterIP None  9093/TCP,9094/TCP,9094/UDP 4m13s service/monitoring-grafana ClusterIP 10.254.108.201  80/TCP 4m20s service/monitoring-kube-state-metrics ClusterIP 10.254.87.45  8080/TCP 4m20s service/monitoring-prometheus-node-exporter ClusterIP 10.254.160.157  9100/TCP 4m20s service/monitoring-prometheus-oper-alertmanager ClusterIP 10.254.129.73  9093/TCP 4m20s service/monitoring-prometheus-oper-operator ClusterIP 10.254.163.131  8080/TCP,443/TCP 4m20s service/monitoring-prometheus-oper-prometheus ClusterIP 10.254.29.59  9090/TCP 4m20s service/prometheus-operated ClusterIP None  9090/TCP 4m3s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/monitoring-prometheus-node-exporter 3 3 3 3 3  4m20s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/monitoring-grafana 1/1 1 1 4m20s deployment.apps/monitoring-kube-state-metrics 1/1 1 1 4m20s deployment.apps/monitoring-prometheus-oper-operator 1/1 1 1 4m20s NAME DESIRED CURRENT READY AGE replicaset.apps/monitoring-grafana-5694798c88 1 1 1 4m20s replicaset.apps/monitoring-kube-state-metrics-5f4d9ddc46 1 1 1 4m20s replicaset.apps/monitoring-prometheus-oper-operator-8bd4fb5b8 1 1 1 4m20s NAME READY AGE statefulset.apps/alertmanager-monitoring-prometheus-oper-alertmanager 1/1 4m13s statefulset.apps/prometheus-monitoring-prometheus-oper-prometheus 1/1 4m13s $ kubectl get servicemonitor -n monitoring NAME AGE monitoring-prometheus-oper-alertmanager 23h monitoring-prometheus-oper-apiserver 23h monitoring-prometheus-oper-coredns 23h monitoring-prometheus-oper-grafana 23h monitoring-prometheus-oper-kube-controller-manager 23h monitoring-prometheus-oper-kube-etcd 23h monitoring-prometheus-oper-kube-proxy 23h monitoring-prometheus-oper-kube-scheduler 23h monitoring-prometheus-oper-kube-state-metrics 23h monitoring-prometheus-oper-kubelet 23h monitoring-prometheus-oper-node-exporter 23h monitoring-prometheus-oper-operator 23h monitoring-prometheus-oper-prometheus 23h $ kubectl get crd -n monitoring NAME CREATED AT alertmanagers.monitoring.coreos.com 2021-03-15T15:13:43Z podmonitors.monitoring.coreos.com 2021-03-15T15:13:43Z probes.monitoring.coreos.com 2021-03-15T15:13:43Z prometheuses.monitoring.coreos.com 2021-03-15T15:13:44Z prometheusrules.monitoring.coreos.com 2021-03-15T15:13:44Z servicemonitors.monitoring.coreos.com 2021-03-15T15:13:44Z thanosrulers.monitoring.coreos.com 2021-03-15T15:13:44Z prometheus-operator 연동  grafana service 수정  $ kubectl edit service/monitoring-grafana -n monitoring spec: clusterIP: 10.254.108.201 externalTrafficPolicy: Cluster ports: - name: service nodePort: 31000 port: 80 protocol: TCP targetPort: 3000 selector: app.kubernetes.io/instance: monitoring app.kubernetes.io/name: grafana sessionAffinity: None type: NodePort = grafana Pod를 외부 port로 접속하기 위해 매핑된 Service를 ClusterIP - NodePort type으로 변경해준다\n redis-cluster Servicemonitor 수정  $ kubectl edit servicemonitor kimdubi-test-redis-cluster labels: app.kubernetes.io/instance: kimdubi-test app.kubernetes.io/managed-by: Helm app.kubernetes.io/name: redis-cluster helm.sh/chart: redis-cluster-4.3.1 release: monitoring = lables 설정에 release: monitoring 을 추가하여 Prometheus-operator 가 인식할 수 있게 함\n각각의 redis Pod은 건드릴 필요 없이 Opertor가 바라보는 Serviemonitor만 수정해주면 되기 때문에 Pod가 확장 되거나 축소되어도 모니터링엔 영향없음\n 외부 접근을 위해 kubernetes node 중 하나에 FIP를 할당해준다   grafana dashboard 접속 확인   default 접속 정보 admin / prom-opertor  ","wordCount":"619","inLanguage":"en","datePublished":"2021-10-06T21:15:29+09:00","dateModified":"2021-10-06T21:15:29+09:00","author":{"@type":"Person","name":"kimdubi"},"mainEntityOfPage":{"@type":"WebPage","@id":"/cloud/k8s_redis_cluster_monitoring/"},"publisher":{"@type":"Organization","name":"kimDuBiA","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>if(localStorage.getItem("pref-theme")==="dark"){document.body.classList.add('dark');}else if(localStorage.getItem("pref-theme")==="light"){document.body.classList.remove('dark')}else if(window.matchMedia('(prefers-color-scheme: dark)').matches){document.body.classList.add('dark');}</script><header class=header><nav class=nav><div class=logo><a href accesskey=h title="Hi (Alt + H)">Hi</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=/ title=Home><span>Home</span></a></li><li><a href=/categories/ title=categories><span>categories</span></a></li><li><a href=/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/cloud/>Clouds</a></div><h1 class=post-title>kubernetes에서 redis cluster를 돌려보자 - monitoring(grafana,prometheus)</h1><div class=post-meta>October 6, 2021&nbsp;·&nbsp;3 min&nbsp;·&nbsp;kimdubi</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#prometheus-operator aria-label="Prometheus Operator?">Prometheus Operator?</a><ul><li><a href=#%ea%b0%9c%eb%85%90 aria-label=개념>개념</a></li></ul></li><li><a href=#%eb%aa%a8%eb%8b%88%ed%84%b0%eb%a7%81%ec%9d%84-%ea%b5%ac%ec%b6%95%ed%95%b4%eb%b3%b4%ec%9e%90 aria-label="모니터링을 구축해보자">모니터링을 구축해보자</a><ul><ul><li><a href=#%ea%b8%b0%ec%a1%b4-redis-cluster-helm-chart-%ec%88%98%ec%a0%95 aria-label="기존 redis-cluster helm chart 수정">기존 redis-cluster helm chart 수정</a></li><li><a href=#prometheus-opertor-%ec%84%a4%ec%b9%98 aria-label="prometheus-opertor 설치">prometheus-opertor 설치</a></li><li><a href=#prometheus-operator-%ec%97%b0%eb%8f%99 aria-label="prometheus-operator 연동">prometheus-operator 연동</a></li></ul></ul></li></ul></div></details></div><div class=post-content><hr><h1 id=prometheus-operator>Prometheus Operator?<a hidden class=anchor aria-hidden=true href=#prometheus-operator>#</a></h1><h2 id=개념>개념<a hidden class=anchor aria-hidden=true href=#개념>#</a></h2><p><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/cloud/k8s/k8s_redis_mon1.png alt></p><ul><li>Operator :</li><li>Service : Kubernetes에서 돌고있는 application, 본문에서는 Redis cluster 를 의미함</li><li>ServiceMonitor : 위의 service, 즉 redis cluster를 scraping 하는 동작을 의미함</li></ul><p>kubernetes 위에서 Pod는 유동적으로 변하지만 ServiceMonitor는 Pod를 label로 구분하여 scraping 하고,<br>Operator는 이 ServiceMonitor만 바라보면 되기 때문에 운영자 입장에서는 application 배포, 운영관리가 자동화되는 효과가 있음</p><h1 id=모니터링을-구축해보자>모니터링을 구축해보자<a hidden class=anchor aria-hidden=true href=#모니터링을-구축해보자>#</a></h1><h3 id=기존-redis-cluster-helm-chart-수정>기존 redis-cluster helm chart 수정<a hidden class=anchor aria-hidden=true href=#기존-redis-cluster-helm-chart-수정>#</a></h3><ul><li>helm values.yaml 수정</li></ul><pre><code>## Prometheus Exporter / Metrics

metrics:
  enabled: true
  
  # Enable this if you're using https://github.com/coreos/prometheus-operator
  serviceMonitor:
    enabled: true
</code></pre><p>=> redis-cluster Pod에 Prometheus Exporter 를 띄우겠다는 설정<br>metrics와 serviceMonitor를 띄워 metric를 수집하도록 설정함</p><ul><li>redis-exporter 확인</li></ul><pre><code>$ kubectl get pod
NAME                                              READY   STATUS      RESTARTS   AGE
kimdubi-test-redis-cluster-0                      2/2     Running     0          3m21s
kimdubi-test-redis-cluster-1                      2/2     Running     0          3m53s
kimdubi-test-redis-cluster-2                      2/2     Running     0          4m28s
kimdubi-test-redis-cluster-3                      2/2     Running     0          5m12s
kimdubi-test-redis-cluster-4                      2/2     Running     0          5m53s
kimdubi-test-redis-cluster-5                      2/2     Running     0          6m18s

$ kubectl exec -it kimdubi-test-redis-cluster-0 -c metrics /bin/sh

kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.

$ ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
1001         1     0  0 07:35 ?        00:00:00 redis_exporter
</code></pre><p>=> Pod 내 container가 1/1 -> 2/2, Pod 하나에 Container 두대가 떴다는 의미<br>Pod 접속 시 -c 옵션을 통해 Container명을 지정해서 접속할 수 있음</p><h3 id=prometheus-opertor-설치>prometheus-opertor 설치<a hidden class=anchor aria-hidden=true href=#prometheus-opertor-설치>#</a></h3><ul><li>prometheus-operator 다운로드</li></ul><pre><code>$ helm fetch stable/prometheus-operator
$ tar zxvpf prometheus-operator-9.3.2.tgz
</code></pre><ul><li>helm chart 수정</li></ul><pre><code>### vi ~/prometheus-operator/charts/grafana

persistence:
  type: pvc
  enabled: true
  storageClassName: kimdubi-test
  accessModes:
    - ReadWriteOnce
  size: 1Gi
</code></pre><ul><li>helm install</li></ul><pre><code>$ helm install --namespace monitoring --create-namespace monitoring ./prometheus-operator
</code></pre><ul><li>구성 확인</li></ul><pre><code>$ helm ls -n monitoring

NAME      	NAMESPACE 	REVISION	UPDATED                             	STATUS  	CHART                    	APP VERSION
monitoring	monitoring	1       	2021-03-17 16:13:07.047828 +0900 KST	deployed	prometheus-operator-9.3.2	0.38.1

$ kubectl get all -n monitoring
NAME                                                         READY   STATUS    RESTARTS   AGE
pod/alertmanager-monitoring-prometheus-oper-alertmanager-0   2/2     Running   0          4m13s
pod/monitoring-grafana-5694798c88-dhnqf                      2/2     Running   0          4m20s
pod/monitoring-kube-state-metrics-5f4d9ddc46-qfmrj           1/1     Running   0          4m20s
pod/monitoring-prometheus-node-exporter-6vd26                1/1     Running   0          4m20s
pod/monitoring-prometheus-node-exporter-r99gf                1/1     Running   0          4m20s
pod/monitoring-prometheus-node-exporter-x4qfw                1/1     Running   0          4m20s
pod/monitoring-prometheus-oper-operator-8bd4fb5b8-7xr9q      2/2     Running   0          4m20s
pod/prometheus-monitoring-prometheus-oper-prometheus-0       3/3     Running   1          4m3s

NAME                                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-operated                     ClusterIP   None             &lt;none&gt;        9093/TCP,9094/TCP,9094/UDP   4m13s
service/monitoring-grafana                        ClusterIP   10.254.108.201   &lt;none&gt;        80/TCP                       4m20s
service/monitoring-kube-state-metrics             ClusterIP   10.254.87.45     &lt;none&gt;        8080/TCP                     4m20s
service/monitoring-prometheus-node-exporter       ClusterIP   10.254.160.157   &lt;none&gt;        9100/TCP                     4m20s
service/monitoring-prometheus-oper-alertmanager   ClusterIP   10.254.129.73    &lt;none&gt;        9093/TCP                     4m20s
service/monitoring-prometheus-oper-operator       ClusterIP   10.254.163.131   &lt;none&gt;        8080/TCP,443/TCP             4m20s
service/monitoring-prometheus-oper-prometheus     ClusterIP   10.254.29.59     &lt;none&gt;        9090/TCP                     4m20s
service/prometheus-operated                       ClusterIP   None             &lt;none&gt;        9090/TCP                     4m3s

NAME                                                 DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/monitoring-prometheus-node-exporter   3         3         3       3            3           &lt;none&gt;          4m20s

NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/monitoring-grafana                    1/1     1            1           4m20s
deployment.apps/monitoring-kube-state-metrics         1/1     1            1           4m20s
deployment.apps/monitoring-prometheus-oper-operator   1/1     1            1           4m20s

NAME                                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/monitoring-grafana-5694798c88                   1         1         1       4m20s
replicaset.apps/monitoring-kube-state-metrics-5f4d9ddc46        1         1         1       4m20s
replicaset.apps/monitoring-prometheus-oper-operator-8bd4fb5b8   1         1         1       4m20s

NAME                                                                    READY   AGE
statefulset.apps/alertmanager-monitoring-prometheus-oper-alertmanager   1/1     4m13s
statefulset.apps/prometheus-monitoring-prometheus-oper-prometheus       1/1     4m13s


$ kubectl get servicemonitor -n monitoring
NAME                                                 AGE
monitoring-prometheus-oper-alertmanager              23h
monitoring-prometheus-oper-apiserver                 23h
monitoring-prometheus-oper-coredns                   23h
monitoring-prometheus-oper-grafana                   23h
monitoring-prometheus-oper-kube-controller-manager   23h
monitoring-prometheus-oper-kube-etcd                 23h
monitoring-prometheus-oper-kube-proxy                23h
monitoring-prometheus-oper-kube-scheduler            23h
monitoring-prometheus-oper-kube-state-metrics        23h
monitoring-prometheus-oper-kubelet                   23h
monitoring-prometheus-oper-node-exporter             23h
monitoring-prometheus-oper-operator                  23h
monitoring-prometheus-oper-prometheus                23h


$ kubectl get crd -n monitoring
NAME                                    CREATED AT
alertmanagers.monitoring.coreos.com     2021-03-15T15:13:43Z
podmonitors.monitoring.coreos.com       2021-03-15T15:13:43Z
probes.monitoring.coreos.com            2021-03-15T15:13:43Z
prometheuses.monitoring.coreos.com      2021-03-15T15:13:44Z
prometheusrules.monitoring.coreos.com   2021-03-15T15:13:44Z
servicemonitors.monitoring.coreos.com   2021-03-15T15:13:44Z
thanosrulers.monitoring.coreos.com      2021-03-15T15:13:44Z
</code></pre><h3 id=prometheus-operator-연동>prometheus-operator 연동<a hidden class=anchor aria-hidden=true href=#prometheus-operator-연동>#</a></h3><ul><li>grafana service 수정</li></ul><pre><code>$ kubectl edit service/monitoring-grafana -n monitoring

spec:
  clusterIP: 10.254.108.201
  externalTrafficPolicy: Cluster
  ports:
  - name: service
    nodePort: 31000
    port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/name: grafana
  sessionAffinity: None
  type: NodePort
</code></pre><p>=> grafana Pod를 외부 port로 접속하기 위해 매핑된 Service를 ClusterIP -> NodePort type으로 변경해준다</p><ul><li>redis-cluster Servicemonitor 수정</li></ul><pre><code>$ kubectl edit servicemonitor kimdubi-test-redis-cluster

  labels:
    app.kubernetes.io/instance: kimdubi-test
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis-cluster
    helm.sh/chart: redis-cluster-4.3.1
    release: monitoring
</code></pre><p>=> lables 설정에 release: monitoring 을 추가하여 Prometheus-operator 가 인식할 수 있게 함<br>각각의 redis Pod은 건드릴 필요 없이 Opertor가 바라보는 Serviemonitor만 수정해주면 되기 때문에 Pod가 확장 되거나 축소되어도 모니터링엔 영향없음</p><ul><li>외부 접근을 위해 kubernetes node 중 하나에 FIP를 할당해준다</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/cloud/k8s/k8s_redis_mon2.png alt></p><ul><li>grafana dashboard 접속 확인</li></ul><p><img loading=lazy src=https://raw.githubusercontent.com/kimdubi/kimdubi.github.io/master/images/cloud/k8s/k8s_redis_mon3.png alt></p><ul><li>default 접속 정보 admin / prom-opertor</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=/tags/kubernetes/>kubernetes</a></li><li><a href=/tags/redis/>redis</a></li></ul></footer><div id=disqus_thread></div><script type=text/javascript>(function(){if(window.location.hostname=="localhost")
return;var dsq=document.createElement('script');dsq.type='text/javascript';dsq.async=true;var disqus_shortname='kimdubia';dsq.src='//'+disqus_shortname+'.disqus.com/embed.js';(document.getElementsByTagName('head')[0]||document.getElementsByTagName('body')[0]).appendChild(dsq);})();</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href>kimDuBiA</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById('menu')
menu.scrollLeft=localStorage.getItem("menu-scroll-position");menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft);}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{if(document.body.className.includes("dark")){document.body.classList.remove('dark');localStorage.setItem("pref-theme",'light');}else{document.body.classList.add('dark');localStorage.setItem("pref-theme",'dark');}})</script><script>document.querySelectorAll('pre > code').forEach((codeblock)=>{const container=codeblock.parentNode.parentNode;const copybutton=document.createElement('button');copybutton.classList.add('copy-code');copybutton.innerText='copy';function copyingDone(){copybutton.innerText='copied!';setTimeout(()=>{copybutton.innerText='copy';},2000);}
copybutton.addEventListener('click',(cb)=>{if('clipboard'in navigator){navigator.clipboard.writeText(codeblock.textContent);copyingDone();return;}
const range=document.createRange();range.selectNodeContents(codeblock);const selection=window.getSelection();selection.removeAllRanges();selection.addRange(range);try{document.execCommand('copy');copyingDone();}catch(e){};selection.removeRange(range);});if(container.classList.contains("highlight")){container.appendChild(copybutton);}else if(container.parentNode.firstChild==container){}else if(codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"){codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);}else{codeblock.parentNode.appendChild(copybutton);}});</script></body></html>